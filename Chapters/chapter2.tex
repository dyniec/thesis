\chapter{Trenowanie sieci i zbieranie danych}
W celu autonomicznej jazdy wytrenowałem konwolucyjną sieć neuronową (CNN -- Convolutional Neural Network)
przetwarzającą obraz z kamery bezpośrednio w porządaną prędkość liniową
oraz obrotową. Takie podejście pozwala szybko zbierać dane uczące. Wystarczy
tylko nagrać obraz z kamery oraz prędkość nadaną przez kierowcę.
\begin{figure}[h]
  \centering
  \fbox{
  \scalebox{0.5}{\includegraphics*[viewport=0 1300 600 2200]{img/model.png}}
  }
\end{figure}
\begin{figure}
  \centering
  \fbox{
  \scalebox{0.5}{\includegraphics*[viewport=0 0000 600 1300]{img/model.png}}
  }
  \caption{Architektura sieci}
  \label{model}
\end{figure}

Wersja sterująca w symulatorze powstała aby odrzucić modele, które nie radzą
sobie w tak prostych warunkach. Dodatkowo zbieranie danych oraz testowanie
modelu jest łatwiejsze. Po pierwsze, nie wymaga przygotowywania sprzętu. Poza tym,
opuszczenie toru przez model nie stwarza zagrożenia uszkodzenia łazika lub jego otoczenia.

Architektura sieci pochodzi z rozwiązania \textit{chauffeur} w konkursie Udacity -- Self Driving Car\footnote
{ Repozytorium dostępne pod \href{https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models}
{https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models}},
ale została zaadaptowana do interfejsu symulatora oraz łazika. Sieć wykorzystana w moim projekcie, w odróżnieniu od rozwiązania konkursowego, zadaje też prędkość. Konwersja z rosbaga (format nagrań ROS-a) do 
naszego formatu oraz sam symulator zostały wykonane przez innych członków projektu.


\section{Trening na symulatorze}
Nagrania do nauki na symulatorze zostały wykonane przeze mnie i dwóch innych
uczestników projektu. Łączna długość nagrań wynosi około 50 minut, z czego 5
minut zostało przeznaczone na zbiór walidacyjny, a reszta była zbiorem uczącym.

Obrazy pochodzą z trzech kamer: jednej skierowanej na wprost, dwóch obróconych
o 20 stopni względem środkowej. Podczas uczenia wykorzystywany był obraz ze wszystkich
kamer. Dla kamery środkowej pożądanym wynikiem były nagrane dane ze sterowania.
Dla kamery lewej wynik był nieznacznie zaburzony w prawo, natomiast analogicznie
dla kamery prawej -- w lewo.
Dodatkowo, obraz z kamery środkowej był dodany w postaci symetrycznego odbicia
ze zmienionym kierunkiem skrętu.

\section{Trening z nagrań łazika}
Do treningu zostało wykorzystane około 150GB nagrań z łazika, zawierających
obraz z kamery oraz informacje o sterowaniu. Jest to kilkanaście objazdów
po podziemnym garażu. Na większości ujęć widać kratkę zazwyczaj zgodną z kierunkiem
jazdy, ale zakręcjącą pod kątem prostym. Około 6GB danych zostało wykorzystanych jako
zbiór walidacyjny.

Nagrania pochodzą z jednej fizycznej kamery. Dla każdego ujęcia zostało
dodane jego odbicie lustrzane z przeciwnym kątem skrętu.

