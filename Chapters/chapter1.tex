\chapter{Preliminaria}
Ta praca 
powstała w ramach przedmiotu "Projekt: autonomiczna jazda łazikiem".

Podczas realizacji przedmiotu powstało wiele rozwiązań dla zadań z 
"konkursów łazikowych".

Żaden spośród łazików biorących udział w University Rover Challenge nie 
używa sieci neuronowych bezpośrednio do nawigacji, ale prawie wszystkie używają
ROS (Robot Operating System) jako podstawy całego oprogramowania. Z tego powodu
rozdziale zostaną poruszone poniższe zagadnienia:
\begin{itemize}
  \item Łazik Aleph 1
  \item Podstawy sieci neuronowych
  \item Architektura ROS
\end{itemize}
\section {Łazik Aleph 1}
Łazik Aleph 1 powstał z inicjatywy 
Koło Pasjonatów Mechaniki i Informatyki ,,Continuum''\footnote{ Strona Koła Pasjonatów Mechaniki i Informatyki ,,Continuum'':
\href{http://continuum.uni.wroc.pl/}{http://continuum.uni.wroc.pl/}}
w roku 2014. Od tego czasu został zaprezentowany na konkursach takich jak 
European Rover Challenge (ERC) oraz University Rover Challenge (URC). Przez ostatnie
dwa lata łazik był doceniany na konkursie URC (w roku 2016 zajął 3 miejsce, a w roku
2017 -- 2 miejsce).

Podczas konkursu URC pojazdy były oceniane w czterech kategoriach\footnote{
Regulamin konkursu URC: \href{http://urc.marssociety.org/home/requirements-guidelines}
{http://urc.marssociety.org/home/requirements-guidelines}}:
\begin{itemize}
  \item Science Cache Task -- pobieranie i badanie próbek w terenie 
  \item Extreme Retrieval and Delivery Task -- transport pakunku w różnych warunkach
    terenowych
  \item Equipment Servicing Task -- zdolności manualne (umiejętność podnoszenia,
    przenoszenia obiektów, obsługa przycisków, przełączników i innych narzędzi)
  \item Autonomous Traversal Task -- umiejętność poruszania się pomiędzy wyznaczonymi
    punktami
\end{itemize}


\section{Podstawy sieci neuronowych}
Głebokie sieci neuronowe (deep neural networks) to popularny model w uczeniu maszynowym.
Celem sieci jest przybliżenie funkcji $f^*$, przyporządkowującej argumentom $x$ wartości
$y$, funkcją $f(x,\theta)=y$ oraz znalezienie parametru $\theta$, który da najlepsze
przybliżenie.
\subsection{Jak działają}
Sieci neuronowe są zazwyczaj złożone z wielu różnych funkcji, nazywanych warstwami. Przykładowo,
$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$, wtedy $f^{(3)}$ jest wartwą wyjściową, której wyniki
powinny odpowiadać funkcji $f^*$. Wyniki dla pozostałych warstw nie są znane, z tego
powodu nazywa się je ukrytymi warstwami.
\subsection{Jak trenować}
W wyniku trenowania chcemy znaleźć takie $\theta$, żeby $f^*(x)\approx f(x,\theta)$.
W tym celu należy zdefiniować funkcję kosztu $L(\theta)$ tzn. odległości modelu 
od celu, zależną od $\theta $,
np. dla problemu regresji, $L$ to średni błąd kwadratowy dla danych uczących. 
Dla takiej funkcji chcielibyśmy teraz znaleźć minimum. Minimum globalne może być trudne
do znalezienia, ale minima lokalne zazwyczaj są wystarczające do większości zastosowań.


Gdyby $L$ byłoby funkcją jednej zmiennej, wystarczyłoby zacząć w losowym miejscu i
wielokrotnie wykonać następujący
krok $x=x-\epsilon L'(x)$, aby dotrzeć do minimum lokalnego. Dla funkcji
wielu zmiennych podobny algorytm działa, ale pochodną należy zastąpić gradientem
$x=x-\epsilon \nabla _x L(x)$.
\subsection{Popularne warstwy}
Warstwa liniowa (linear lub dense)

 jest najbardziej podstawową warstwą. Każdy 
element wyjściowy ($m$ wartości) jest kombinacją wszystkich wejść ($n$ wartości)
danej warstwy (powiększoną o stałą). Zatem taka warstwa jest parametryzowana
macierzą rozmiaru $n \cdot m$ oraz wektorem rozmiaru $m$.

Dwie takie sąsiednie warstwy liniowe można by zredukować do jednej, ponieważ
$W_2 \cdot (W_1 \cdot x + b_1) +b_2 = W \cdot x + b$, gdy $W=W_2 \cdot W_1$ oraz
$b = W_2 \cdot b_1 +b_2$. Zatem dowolnie głęboką sieć złożoną z takich warstw
możnaby zredukować do jednej takiej warstwy, ale po każdej funkcji liniowej aplikuje
się funkcję nieliniową np. $tanh$ lub $relu$ ($\max(0,x)$). Dzięki temu sieci neuronowe
są w stanie pokryć znacznie większą przestrzeń funkcji niż tylko liniowe.

Inną warstwą, już specjalizowaną w przetwarzaniu danych położonych na pewnej kracie,
jest warstwa konwolucyjna. Przykładowym wejściem dla takiej warstwy może być
dwuwymiarowa siatka pikseli. Natomiast wyjściem jest obraz o zbliżonej (lub tej samej)
rozdzielczości, którego wartość jest kombinancją liniową spójnego bloku pikseli z 
wejścia. Ważną cechą takiej warstwy jest fakt, że wszystkie wyjścia korzystają
z tych samych parametrów, co powoduje że znajdują te same wzorce położone w 
innych miejscach.

Kolejnym typem warstw specjalizowanym w przetwarzaniu obrazów jest pooling.
Dzieli ona wejście na spójne rozłączne bloki, na każdym z nich osobno aplikuje
funkcję np. $max$ lub $avg$. Taka operacja powoduje niewrażliwość na małe 
przemieszczenia wejść. Dodatkowo zmniejsza to rozmiar wejścia w kolejnych warstwach,
co zmniejsza liczbę parametrów.
\subsection{Uwagi}
Konwolucyjne sieci neuronowe bardzo dobrze radzą sobie z widzeniem maszynowym, są w stanie
klasyfikować bezproblemowo obrazki\cite{cnn}.
Ale w przypadku patrzenia na tylko jedną kratkę nie są wstanie wyciągnąć wniosków.
Co w przypadku nawigacji oznacza, że proces sterowania jest tylko ciągiem
spontanicznych decyzji bez planowania trasy. Dodatkową konsekwencją takiej architektury
jest ukryte założenie, że dla każdego obrazu z kamery jest tylko jedna poprawna
odpowiedź.


\section{Architektura ROS}
ROS (Robot Operating System) to rozbudowany framework przeznaczony do programowania robotów.
Składa się na niego wiele bibliotek oraz narzędzi mających na celu zbudowanie
klastra komputerów tworzących spójny system.
Dostarcza on abstrakcję nad sprzętem, środki komunikacji między procesami oraz
oraz inne funkcjonalności gwarantowane przez typowy system operacyjny.
Ze względu na modułową budowę oraz architekturę peer-to-peer procesy mogą
bezproblemowo działać na różnych komputerach.
\subsection{Node}
Podstawową jednostką w ROS-ie jest wierzchołek (node), jego głównym zdaniem jest
wykonywanie obliczeń. Wierzchołki razem tworzą graf i komunikują się za 
pomocą tematów (topic).

Taka architektura (inspirowana budową mikrojądra) w porównaniu do architektury monolitycznej, zapewnia lepszą ochronę przed błędami. Dodatkowo pojedyńczy element można
bezproblemowo przepisać także w innym języku programowania.
\subsection{Topic}
Tematy (topic) pozwalają bezproblemowo zapewnić komunikację międzyprocesową
w ROS-ie. Każdy node może zadeklarować chęć nadawania bądź nasłuchiwania na
danym temacie. Przykładowo, moduł jazdy autonomicznej może zasubskrybować
obraz z kamery Kinect i publikować na temacie reprezentującym kierunek ruchu.
Tematy są otypowane, co gwarantuje że wszystkie wiadomości wysłane na tym
samym temacie mają taką samą strukturę.
\subsection{Gotowe moduły}
ROS dostarcza wiele gotowych modułów pozwalających szybko rozpocząć projekt.
Jednym z nich jest Odom, który zbiera informacje o położeniu i prędkości 
z wielu źródeł danych i łączy je w nowy strumień danych (o większej pewności). Przykładowo, 
dane może zbierać z czujnika GPS, prędkości obrotowej kół oraz akcelerometru.

Inny moduł potrafi tworzyć mapy na podstawie obrazu z kamery oraz mapy głębokości.
Wynik tej rekonstrukcji można obejrzeć za pomocą innych usług służących do 
wizualizacji różnych typów danych takich jak obraz, wartości zmieniających się
w czasie, chmury punktów lub mapy terenu.

% \section{Autonomia Aleph 1}
%Co zostało zrobione na przedmiocie:
%\begin{itemize}
%  \item Sprzęt (mnóstwo)
%  \item Mapa 3d (RTAB\_MAP)
%  \item Rozpoznawanie klawiatur/piłek tenisowych
%  \item Symulator
%  \item kilka sieci obraz->kierownica
%  \item wrappery/konwertery różnych protokołów/formatów
%\end{itemize}
